# Photolithography-Mask-Simulator

#### Video Demo:  https://youtu.be/_Tgt35NTyAo

#### Description:

This project is a webapp built on a Flask framework that uses either Gaussian or Airy kernels to estimate the intensity experienced by a wafer from a given photolithography mask layout. The central figure of merit for this process is the Critical Dimension (CD) or Rayleigh Criterion. This value is calculated using CD = k*wavelength/NA, all of which values can be adjusted using the slider. The value of CD represents the smallest dimension which can be physically realized by a photolithography process. Therefore, if the CD is ~300 nm or greater, the feature will just look like a blur in the 1000 nm by 1000 nm output image, as there will be high amounts of diffraction which obscur the input mask shape. You can adjust sliders to investigate the effects of wavelength, k1 factor, and numerical aperature on diffraction. The Gaussian kernel can be used in automatic mode wherein the simulated output dynamically changes as slider values are changed. With the current Airy function implementation, manual mode is recommended.

The webapp consists of a single HTML page, `index.html`, which handles both input and output. I chose to have a single page for both input and output so that the tool could be used interactively, as redirects would greatly limit iteration of photolithography parameters. The webpage elements are styled using Boostrap and `CSS` implmented in `styles.css`. The main section consists of two `canvas` elements, and a box containing sliders and buttons. The "input canvas" on the left employs JavaScript events to track the mouse position, and whether the mouse is being clicked. If the mouse is on the canvas, and a mouse button is being clicked, a 15x15 px block will be drawn at the mouse's location, with the cursor overlapping with the top-leftmost pixel. Since photolithography masks use an empty hole in the middle of a substrate through which light passes, I chose to have the background black and the user draws the hole in white. The second canvas is initialized as blank (black) and stores the simulated output. For convenience of interacting with the canvas elements, I added a reset button and download button to reset the input (left) canvas to its initialized state, and download the contents of the output canvas (right), respectively. The rightmost block of the main section is the "Parameters and Settings" box. Here you can adjust both physical parameters (Wavelength, k1-factor, Numerical Aperature), as well as calculation parameters such as the choice of Point Spread Function used to simulate the output in the backend. These sliders are limited to current experimental/industrial limits and/or theoretical maxima and minima. There is a `checkbox` stylized as a slider which the user can toggle between automatic and manual calculations on parameter changes. The parameter sliders all have `EventListener`'s to update the labels next to the sliders Finally, there are two `async` JavaScript functions to update the critical dimension at the bottom of the page on parameter changes, and to pack the parameters and image into a JSON and `POST` it to the `\simulate` route in Flask. This is done using `fetch` to `await` the response from the framework containing the simulated output image.

The Flask framework, `app.py`,  is fairly straightforward and only contains two routes. The `/` route renders the homepage `index.html`. The `\simulate` route is only called via `POST` and generally gets a `JSON` from the webpage containing the values of the sliders and the image, before calculated the simulated response and returning the output image as a `JSON`. I chose to use `cffi` in this backend, due to the speed of precompiled C libraries. I also found in my research that I could have used `ctypes`, but `cffi` seemed to provide faster computation, which I desired for interactive level functionality. The `cffi` library allowed me to declare the C function header, relevant C library, and compute simply within the flask framework. However, I needed to handle unit conversions to appropriately format the images received and transmitted, to and from both the HTML elements and the library. From the HTML I received a `base_64`-encoded image which I wanted to convert to `uint8` to speed up calculations. Since I'm really just dealing with grayscale data `uint8` should be fine for each pixel. To make this conversion happen I used the `base_64`, `PIL`, and `BytesIO` libraries in Python. I used these since I found a straightforward implementation when trying to research how to convert image types (https://stackoverflow.com/questions/26070547/decoding-base64-from-post-to-use-in-pil/26079673#26079673). Once I had a `PIL` image object, I binarized it (values of 0 & 255 thresholded at 128, handled by `PIL` library), flattened it to pass to my C library as a simple `uint8_t *input`. The output buffer is changed in place by the library, and it is then repackaged into a JSON using `jsonify` and sent back to the eagerly awaiting JavaScript within `index.html`. 

The library `psf_sim.so` which is called by the framework was compiled from  `psf_sim.c` using `gcc -O3 -fPIC -shared psf_sim.c -o psf_sim.so`. The library is just one function with two branches, the Gaussian branch and Airy branch for the different PSF methods. Both methods use a `MASK_WIDTH` of 1000 nm to tie the pixels to actual physically relevant values. In the Gaussian branch, I use the CD as the standard deviation for the kernel, and I use 3 standard deviations to determine the kernel size. Initially I had a slider for kernel size, but the effect of the kernel size dominated over the photolithography parameters. I `malloc` memory for the kernel and build it according to its definition (staff.fnwi.uva.nl/r.vandenboomgaard/ComputerVision/LectureNotes/IP/LocalStructure/GaussianDerivatives.html), although I chose to normalize the maximal (central) element to unity since the discrete nature make the continuous normalization prefactor incorrect. Also, I chose a 1D kernel which will pass over the image twice, since this avoids having 4 nested `for` loops (image_x, image_y, kernel_x, kernel_y) yield O(n^2*m^2) complexity where n is the height and width of the canvas in pixels, and m is the size of the kernel. The resulting method is instead O(n^2*m), which allows for interactive speeds. During the process of convolution, I store the temporary outputs in float buffers to keep precision, only casting back to a `uint8_t` buffer at the final step. I free the temporary buffers and kernel after the output image has been written. 
The Airy branch is much slower, and slightly more complicated. This approach is more physically valid due to actually simulating the physical process of diffraction, but generally this PSF is well approximated by a Gaussian. I dynamically determine the kernel size in this approach using the well-known zeros of the Bessel function of the first kind (J) to have the kernel contain the entire Gaussian-like envelope. I initially tried to used the third zero, which captures roughly 10% (absolute) more of the intensity than the gaussian function, but this yielded a large kernel which was too slow. Eventually I ended up just using the first zero of the Bessel function. The convolution this time could be decomposed so simply into two 1D passes, and without implementing FFT methods I did not see a way to achieve faster than O(n^2*m^2) time complexity. The price of enhanced accuracy is reduced computational speeds. This ends up being too slow for interactive level display, and therefore if manual mode is not used while running this calculation, the `POST` requests stack up and the image lags on the output canvas.
